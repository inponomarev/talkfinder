<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title></title>

  <meta name="description" content="JUG Talks Finder">
  <meta name="author" content="Ivan Ponomarev">
  <link rel="stylesheet" href="/css/foundation.css">
  <link rel="stylesheet" href="/css/font-awesome.css">
  <link rel="stylesheet" href="/css/coderay.css">
  <link rel="stylesheet" href="/css/asciidoctor.css">
  <link rel="stylesheet" href="/css/custom.css">
  <!-- Yandex.Metrika counter -->
  <script type="text/javascript">
    (function (m, e, t, r, i, k, a) {
      m[i] = m[i] || function () { (m[i].a = m[i].a || []).push(arguments) };
      m[i].l = 1 * new Date(); k = e.createElement(t), a = e.getElementsByTagName(t)[0], k.async = 1, k.src = r, a.parentNode.insertBefore(k, a)
    })
      (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(64729483, "init", {
      clickmap: true,
      trackLinks: true,
      accurateTrackBounce: true
    });
  </script>
  <noscript>
    <div><img src="https://mc.yandex.ru/watch/64729483" style="position:absolute; left:-9999px;" alt="" /></div>
  </noscript>
  <!-- /Yandex.Metrika counter -->
  <script src="/js/vendor/modernizr.js"></script>
  <script src="/js/toc.js"></script>
  <script src="/js/simple-jekyll-search.min.js"></script>
</head>
<body>
  <nav class="top-bar">
    <ul class="title-area">
      <li class="name">
        <h1>
          <a href="/ru/events.html">События</a>
          <a href="/ru/speakers.html">Спикеры</a>
          <a href="/ru/search.html">Поиск</a>
          <a href="/ru/about.html">О проекте</a>
          <a class="lang" href="/en/talk/2537.html">EN</a>
        </h1>
      </li>
    </ul>
  </nav>
  <div class="main-content">
    <div class="paragraph">
<p><span class="image"><a class="image" href="../evttype/5.html"><img src="/images/events/hydra.jpg" alt="hydra" width="80"></a></span> <a href="../event/46.html">Hydra 2020 (06.07.2020&#8201;&#8212;&#8201;09.07.2020)
</a></p>
</div>
<div class="sect3">
<h4 id="distributed_and_concurrent_optimization_for_machine_learning">Distributed and concurrent optimization for machine learning</h4>
<div class="paragraph event-speakers">
<p><span class="image"><a class="image" href="../speaker/1248.html"><img src="/images/speakers/1248.jpg" alt="img" width="100" height="100"></a></span></p>
</div>
<div class="paragraph">
<p><em><a href="../speaker/1248.html">Dan Alistarh</a></em></p>
</div>
<div class="paragraph">
<p>The goal of this talk is to provide an overview of the role of distributed</p>
</div>
<div class="paragraph">
<p>computing in machine learning, with an eye towards the intriguing trade-offs between synchronization and communication costs of distributed machine learning algorithms, on the one hand, and their convergence, on the other.</p>
</div>
<div class="paragraph">
<p>&lt;p&gt;Machine learning has made considerable progress over the past decade,</p>
</div>
<div class="paragraph">
<p>matching and even surpassing human performance on a varied set of narrow computational tasks. This progress has been enabled by the widespread availability of large datasets, as well as by improved algorithms and models. Distribution, implemented either through single-node concurrency or through multi-node parallelism has been the third key ingredient for these advances.&lt;/p&gt;
&lt;p&gt;The goal of this talk is to provide an overview of the role of distributed</p>
</div>
<div class="paragraph">
<p>computing in machine learning, with an eye towards the intriguing trade-offs between synchronization and communication costs of distributed machine learning algorithms, on the one hand, and their convergence, on the other. The focus will be on parallelization strategies for the fundamental stochastic gradient descent (SGD) algorithm, which is a key tool when training machine learning models, from venerable linear regression to state-of-the-art neural network architectures.</p>
</div>
<div class="paragraph">
<p>Along the way, we will provide an overview of the ongoing research and open problems in distributed machine learning. The talk will assume no prior knowledge of machine learning or optimization, beyond familiarity with basic concepts in algebra and analysis.&lt;/p&gt;</p>
</div>
<div class="ulist">
<div class="title">Ссылки:</div>
<ul>
<li>
<p><a href="https://assets.ctfassets.net/oxjq45e8ilak/7K92rVO6GkdW9HDOLeaiOL/e0f4dc95b6c6d64f16d84849acdd93f0/Hydra-Talk__1_.pdf" class="bare">https://assets.ctfassets.net/oxjq45e8ilak/7K92rVO6GkdW9HDOLeaiOL/e0f4dc95b6c6d64f16d84849acdd93f0/Hydra-Talk__1_.pdf</a></p>
</li>
</ul>
</div>
</div>
  </div>
</body>
</html>